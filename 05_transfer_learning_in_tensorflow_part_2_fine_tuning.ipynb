{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPKDB4AQJCx+lkGo+fh6Uzt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mjaliz/tfztm/blob/main/05_transfer_learning_in_tensorflow_part_2_fine_tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transfer Learning with TensorFlow Part 2: Fine-tuning"
      ],
      "metadata": {
        "id": "PbDgSp7tDQK6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if we're using a GPU\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ObZ5Ns1sELqf",
        "outputId": "283ec65c-2b1d-4b0e-e50c-ff25f6ff56b7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: nvidia-smi: command not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FCRgoLdUEkYr",
        "outputId": "b4067d21-184d-440f-d14d-2b4a8e3eb313"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-09-27 17:35:36--  https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 10246 (10K) [text/plain]\n",
            "Saving to: ‘helper_functions.py’\n",
            "\n",
            "\rhelper_functions.py   0%[                    ]       0  --.-KB/s               \rhelper_functions.py 100%[===================>]  10.01K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-09-27 17:35:37 (73.8 MB/s) - ‘helper_functions.py’ saved [10246/10246]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import helper functions we're going to use in this notebook\n",
        "from helper_functions import create_tensorboard_callback, plot_loss_curves, unzip_data, walk_through_dir"
      ],
      "metadata": {
        "id": "gsim6WwJGXcQ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Let's get some data"
      ],
      "metadata": {
        "id": "cdIHZ5RXG22p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get 10 % of training data of 10 classes of Food101\n",
        "!wget https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_10_percent.zip\n",
        "\n",
        "unzip_data(\"10_food_classes_10_percent.zip\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NuubR2r-IcKr",
        "outputId": "249fda7c-a853-403e-8f46-65be28b76bd9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-09-27 17:35:45--  https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_10_percent.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 173.194.211.207, 173.194.212.207, 173.194.213.207, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|173.194.211.207|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 168546183 (161M) [application/zip]\n",
            "Saving to: ‘10_food_classes_10_percent.zip’\n",
            "\n",
            "10_food_classes_10_ 100%[===================>] 160.74M   165MB/s    in 1.0s    \n",
            "\n",
            "2023-09-27 17:35:46 (165 MB/s) - ‘10_food_classes_10_percent.zip’ saved [168546183/168546183]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check out how many iamges and subdirectories are in our dataset\n",
        "walk_through_dir(\"10_food_classes_10_percent\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ps7E9J1LJWUG",
        "outputId": "7e550a9f-7eba-4862-f9e2-64e0e7ab09ab"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 2 directories and 0 images in '10_food_classes_10_percent'.\n",
            "There are 10 directories and 0 images in '10_food_classes_10_percent/test'.\n",
            "There are 0 directories and 250 images in '10_food_classes_10_percent/test/ice_cream'.\n",
            "There are 0 directories and 250 images in '10_food_classes_10_percent/test/pizza'.\n",
            "There are 0 directories and 250 images in '10_food_classes_10_percent/test/fried_rice'.\n",
            "There are 0 directories and 250 images in '10_food_classes_10_percent/test/chicken_curry'.\n",
            "There are 0 directories and 250 images in '10_food_classes_10_percent/test/chicken_wings'.\n",
            "There are 0 directories and 250 images in '10_food_classes_10_percent/test/grilled_salmon'.\n",
            "There are 0 directories and 250 images in '10_food_classes_10_percent/test/hamburger'.\n",
            "There are 0 directories and 250 images in '10_food_classes_10_percent/test/ramen'.\n",
            "There are 0 directories and 250 images in '10_food_classes_10_percent/test/steak'.\n",
            "There are 0 directories and 250 images in '10_food_classes_10_percent/test/sushi'.\n",
            "There are 10 directories and 0 images in '10_food_classes_10_percent/train'.\n",
            "There are 0 directories and 75 images in '10_food_classes_10_percent/train/ice_cream'.\n",
            "There are 0 directories and 75 images in '10_food_classes_10_percent/train/pizza'.\n",
            "There are 0 directories and 75 images in '10_food_classes_10_percent/train/fried_rice'.\n",
            "There are 0 directories and 75 images in '10_food_classes_10_percent/train/chicken_curry'.\n",
            "There are 0 directories and 75 images in '10_food_classes_10_percent/train/chicken_wings'.\n",
            "There are 0 directories and 75 images in '10_food_classes_10_percent/train/grilled_salmon'.\n",
            "There are 0 directories and 75 images in '10_food_classes_10_percent/train/hamburger'.\n",
            "There are 0 directories and 75 images in '10_food_classes_10_percent/train/ramen'.\n",
            "There are 0 directories and 75 images in '10_food_classes_10_percent/train/steak'.\n",
            "There are 0 directories and 75 images in '10_food_classes_10_percent/train/sushi'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create training and test directory paths\n",
        "train_dir = \"10_food_classes_10_percent/train\"\n",
        "test_dir = \"10_food_classes_10_percent/test\""
      ],
      "metadata": {
        "id": "2udj1xViJlfo"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "IMG_SIZE = (224, 224)\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "train_data_10_percent = tf.keras.utils.image_dataset_from_directory(directory=train_dir,\n",
        "                                                                    image_size=IMG_SIZE,\n",
        "                                                                    label_mode=\"categorical\",\n",
        "                                                                    batch_size=BATCH_SIZE)\n",
        "\n",
        "test_data = tf.keras.utils.image_dataset_from_directory(directory=test_dir,\n",
        "                                                        image_size=IMG_SIZE,\n",
        "                                                        label_mode=\"categorical\",\n",
        "                                                        batch_size=BATCH_SIZE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OewyrFLRKG5-",
        "outputId": "5c655b8d-12e2-470c-873a-2d97eda221ef"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 750 files belonging to 10 classes.\n",
            "Found 2500 files belonging to 10 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_10_percent"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DN-Yh9vJL_zC",
        "outputId": "d04351a5-32bc-4514-b526-aa7f11a2b348"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_BatchDataset element_spec=(TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 10), dtype=tf.float32, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check out the class names of our dataset\n",
        "train_data_10_percent.class_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jH4zMSzPPlVq",
        "outputId": "aeca41e7-f30a-4a9c-bdd5-a222a56525c7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['chicken_curry',\n",
              " 'chicken_wings',\n",
              " 'fried_rice',\n",
              " 'grilled_salmon',\n",
              " 'hamburger',\n",
              " 'ice_cream',\n",
              " 'pizza',\n",
              " 'ramen',\n",
              " 'steak',\n",
              " 'sushi']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# See an example of a batch of data\n",
        "for images, labels in train_data_10_percent.take(1):\n",
        "  print(images, labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UlmaAC4HP9d6",
        "outputId": "2bad1f54-1260-4ca4-cbbd-974e7fdbf230"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[[[6.35714293e+00 1.35714281e+00 0.00000000e+00]\n",
            "   [5.00000000e+00 1.00000000e+00 0.00000000e+00]\n",
            "   [5.14285660e+00 1.78571415e+00 0.00000000e+00]\n",
            "   ...\n",
            "   [5.79596806e+00 1.42345536e+00 0.00000000e+00]\n",
            "   [1.05714674e+01 6.88783884e-01 0.00000000e+00]\n",
            "   [1.63571777e+01 2.00003481e+00 0.00000000e+00]]\n",
            "\n",
            "  [[6.35714293e+00 1.35714281e+00 0.00000000e+00]\n",
            "   [5.00000000e+00 1.00000000e+00 0.00000000e+00]\n",
            "   [5.14285660e+00 1.78571415e+00 0.00000000e+00]\n",
            "   ...\n",
            "   [8.67353153e+00 3.41816425e-01 0.00000000e+00]\n",
            "   [1.57806635e+01 2.85727262e-01 5.10297250e-03]\n",
            "   [2.18316650e+01 2.47452211e+00 9.18317065e-02]]\n",
            "\n",
            "  [[6.35714293e+00 1.35714281e+00 0.00000000e+00]\n",
            "   [5.00000000e+00 1.00000000e+00 0.00000000e+00]\n",
            "   [5.14285660e+00 1.78571415e+00 0.00000000e+00]\n",
            "   ...\n",
            "   [1.85663986e+01 2.78571415e+00 6.17363930e-01]\n",
            "   [2.68724766e+01 3.85715580e+00 1.53060448e+00]\n",
            "   [3.22092094e+01 4.78571415e+00 1.56628418e+00]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[1.78570557e+00 3.21429443e+00 0.00000000e+00]\n",
            "   [1.07142830e+00 2.07142830e+00 0.00000000e+00]\n",
            "   [1.16836560e+00 1.61733997e+00 0.00000000e+00]\n",
            "   ...\n",
            "   [1.63572083e+01 3.00000000e+00 1.44897830e+00]\n",
            "   [1.82296352e+01 1.58192843e-01 0.00000000e+00]\n",
            "   [3.07247372e+01 9.57165527e+00 4.49511433e+00]]\n",
            "\n",
            "  [[1.00000000e+00 4.00000000e+00 0.00000000e+00]\n",
            "   [1.00510073e+00 2.00510073e+00 0.00000000e+00]\n",
            "   [1.00000000e+00 1.00000000e+00 0.00000000e+00]\n",
            "   ...\n",
            "   [1.67551346e+01 2.07141113e+00 1.53008252e-02]\n",
            "   [2.10715332e+01 2.00516272e+00 0.00000000e+00]\n",
            "   [3.46481743e+01 1.17399845e+01 5.78588867e+00]]\n",
            "\n",
            "  [[0.00000000e+00 1.35714722e+00 0.00000000e+00]\n",
            "   [0.00000000e+00 2.00000000e+00 0.00000000e+00]\n",
            "   [1.93367028e+00 1.50509858e+00 1.99489284e+00]\n",
            "   ...\n",
            "   [1.62908478e+01 4.94887352e-01 0.00000000e+00]\n",
            "   [2.35969868e+01 3.33169317e+00 4.59264629e-02]\n",
            "   [3.38419189e+01 1.08419209e+01 4.84192038e+00]]]\n",
            "\n",
            "\n",
            " [[[1.24663269e+02 1.10663269e+02 9.76632690e+01]\n",
            "   [1.83015305e+02 1.69015305e+02 1.56015305e+02]\n",
            "   [1.99505096e+02 1.86505096e+02 1.70505096e+02]\n",
            "   ...\n",
            "   [2.53571434e+01 1.09286146e+01 1.09286146e+01]\n",
            "   [2.29285583e+01 8.92855835e+00 7.92855835e+00]\n",
            "   [2.23571777e+01 8.35717773e+00 7.35717773e+00]]\n",
            "\n",
            "  [[1.75744904e+02 1.61744904e+02 1.48744904e+02]\n",
            "   [1.93127548e+02 1.79127548e+02 1.66127548e+02]\n",
            "   [2.00714279e+02 1.87714279e+02 1.71500000e+02]\n",
            "   ...\n",
            "   [2.23418159e+01 7.91328812e+00 7.91328812e+00]\n",
            "   [2.20000000e+01 8.00000000e+00 7.00000000e+00]\n",
            "   [2.30000000e+01 9.00000000e+00 8.00000000e+00]]\n",
            "\n",
            "  [[1.95158157e+02 1.81515305e+02 1.65729599e+02]\n",
            "   [1.95755096e+02 1.82112244e+02 1.66112244e+02]\n",
            "   [2.01214294e+02 1.87571442e+02 1.71403061e+02]\n",
            "   ...\n",
            "   [2.22601776e+01 8.26017761e+00 8.04591370e+00]\n",
            "   [2.18571548e+01 8.28572750e+00 8.28572750e+00]\n",
            "   [2.25714283e+01 9.00000000e+00 8.21428585e+00]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[4.35050507e+01 1.92907867e+01 1.50765228e+01]\n",
            "   [4.14999542e+01 1.72856922e+01 1.30714283e+01]\n",
            "   [4.16734657e+01 1.72908306e+01 1.30765667e+01]\n",
            "   ...\n",
            "   [2.95255623e+01 1.07398272e+01 7.16835499e+00]\n",
            "   [2.91428223e+01 1.04591427e+01 6.87236309e+00]\n",
            "   [3.12857971e+01 1.39285889e+01 1.01428528e+01]]\n",
            "\n",
            "  [[2.78571815e+01 8.19392681e+00 2.52555418e+00]\n",
            "   [3.00714283e+01 9.21431160e+00 4.14286995e+00]\n",
            "   [2.81122265e+01 7.11222649e+00 2.11222625e+00]\n",
            "   ...\n",
            "   [3.61581917e+01 1.51581907e+01 1.21581907e+01]\n",
            "   [3.50051041e+01 1.41377792e+01 1.10714417e+01]\n",
            "   [3.62602196e+01 1.72602196e+01 1.32602186e+01]]\n",
            "\n",
            "  [[2.97703152e+01 1.17703161e+01 7.77031612e+00]\n",
            "   [2.69488697e+01 8.94886971e+00 4.94886971e+00]\n",
            "   [2.99439697e+01 1.12246094e+01 7.22460938e+00]\n",
            "   ...\n",
            "   [3.53416672e+01 1.36988754e+01 1.09131393e+01]\n",
            "   [3.61225662e+01 1.51225681e+01 1.21225681e+01]\n",
            "   [3.51430664e+01 1.41430664e+01 1.11430664e+01]]]\n",
            "\n",
            "\n",
            " [[[2.94183693e+01 2.24183693e+01 1.44183683e+01]\n",
            "   [2.65204086e+01 1.95204086e+01 1.35204105e+01]\n",
            "   [1.90000000e+01 1.20000000e+01 6.00000048e+00]\n",
            "   ...\n",
            "   [6.00000000e+00 9.00000000e+00 1.40000000e+01]\n",
            "   [5.07141113e+00 8.07141113e+00 1.30714111e+01]\n",
            "   [5.00000000e+00 8.00000000e+00 1.30000000e+01]]\n",
            "\n",
            "  [[2.78367310e+01 2.08367310e+01 1.48367319e+01]\n",
            "   [4.32857132e+01 3.62857132e+01 3.02857132e+01]\n",
            "   [5.58571472e+01 4.88571472e+01 4.28571472e+01]\n",
            "   ...\n",
            "   [6.00000000e+00 9.00000000e+00 1.40000000e+01]\n",
            "   [5.07141113e+00 8.07141113e+00 1.30714111e+01]\n",
            "   [5.00000000e+00 8.00000000e+00 1.30000000e+01]]\n",
            "\n",
            "  [[2.99438763e+01 2.27295914e+01 1.73724480e+01]\n",
            "   [3.26428604e+01 2.54285698e+01 2.00714283e+01]\n",
            "   [3.82346916e+01 3.04030609e+01 2.68979588e+01]\n",
            "   ...\n",
            "   [6.00000000e+00 9.00000000e+00 1.40000000e+01]\n",
            "   [5.07141113e+00 8.07141113e+00 1.30714111e+01]\n",
            "   [5.00000000e+00 8.00000000e+00 1.30000000e+01]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[1.45285645e+02 1.47071381e+02 1.07071388e+02]\n",
            "   [1.45785645e+02 1.47571381e+02 1.07571388e+02]\n",
            "   [1.48260162e+02 1.50045898e+02 1.10045891e+02]\n",
            "   ...\n",
            "   [1.53999969e+02 1.50168320e+02 1.11831650e+02]\n",
            "   [1.53214264e+02 1.49214264e+02 1.12000000e+02]\n",
            "   [1.53785736e+02 1.49785736e+02 1.14357208e+02]]\n",
            "\n",
            "  [[1.48071396e+02 1.53071396e+02 1.12071388e+02]\n",
            "   [1.45862198e+02 1.50862198e+02 1.09862206e+02]\n",
            "   [1.46127533e+02 1.51127533e+02 1.10127533e+02]\n",
            "   ...\n",
            "   [1.56714264e+02 1.54443832e+02 1.17285736e+02]\n",
            "   [1.54071381e+02 1.51071381e+02 1.17938759e+02]\n",
            "   [1.52214264e+02 1.49214264e+02 1.16357147e+02]]\n",
            "\n",
            "  [[1.39642899e+02 1.46642899e+02 1.04642891e+02]\n",
            "   [1.38474518e+02 1.45474518e+02 1.03474525e+02]\n",
            "   [1.40000000e+02 1.47000000e+02 1.05000000e+02]\n",
            "   ...\n",
            "   [1.54709167e+02 1.51709167e+02 1.17290833e+02]\n",
            "   [1.53045898e+02 1.50045898e+02 1.18954094e+02]\n",
            "   [1.52301117e+02 1.48301117e+02 1.19301117e+02]]]\n",
            "\n",
            "\n",
            " ...\n",
            "\n",
            "\n",
            " [[[1.51229584e+02 1.22285713e+02 7.49030609e+01]\n",
            "   [1.44239792e+02 1.17454086e+02 7.90051041e+01]\n",
            "   [1.32433670e+02 1.11362244e+02 8.41581650e+01]\n",
            "   ...\n",
            "   [1.95714073e+01 7.57140684e+00 7.57140684e+00]\n",
            "   [2.04285851e+01 6.42858458e+00 6.42858458e+00]\n",
            "   [2.20714626e+01 8.07146358e+00 8.07146358e+00]]\n",
            "\n",
            "  [[1.35663269e+02 1.15306122e+02 7.98775482e+01]\n",
            "   [1.27168365e+02 1.09602036e+02 8.08877563e+01]\n",
            "   [1.27903061e+02 1.13102043e+02 9.44336853e+01]\n",
            "   ...\n",
            "   [2.05867767e+01 7.01530457e+00 7.01530457e+00]\n",
            "   [2.19285717e+01 5.92857170e+00 6.92857170e+00]\n",
            "   [2.29285717e+01 6.92857170e+00 7.92857170e+00]]\n",
            "\n",
            "  [[1.23964287e+02 1.16821426e+02 9.87602005e+01]\n",
            "   [1.38469391e+02 1.32198990e+02 1.17612251e+02]\n",
            "   [1.49801025e+02 1.44086731e+02 1.35877563e+02]\n",
            "   ...\n",
            "   [2.16887932e+01 5.26022148e+00 5.26022148e+00]\n",
            "   [2.44285717e+01 6.00000000e+00 6.21428585e+00]\n",
            "   [2.52142868e+01 6.78571415e+00 7.00000000e+00]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[2.03418388e+01 9.34183788e+00 5.34183836e+00]\n",
            "   [2.32041416e+01 1.22041416e+01 8.20414162e+00]\n",
            "   [2.10458698e+01 1.00458698e+01 8.04586983e+00]\n",
            "   ...\n",
            "   [5.25662537e+01 2.23519917e+01 2.01377277e+01]\n",
            "   [5.12142029e+01 2.17856750e+01 1.78570557e+01]\n",
            "   [5.70562057e+01 2.61327362e+01 2.09031448e+01]]\n",
            "\n",
            "  [[2.15204258e+01 1.05204248e+01 6.52042484e+00]\n",
            "   [2.18622589e+01 1.08622599e+01 6.86225986e+00]\n",
            "   [1.82143250e+01 7.21432495e+00 5.21432495e+00]\n",
            "   ...\n",
            "   [5.51734962e+01 2.71734982e+01 2.41734982e+01]\n",
            "   [5.07907791e+01 2.17907791e+01 1.77907791e+01]\n",
            "   [5.15204391e+01 2.05204372e+01 1.75204372e+01]]\n",
            "\n",
            "  [[2.00611401e+01 9.06114006e+00 5.18870354e+00]\n",
            "   [2.30714283e+01 1.20714283e+01 8.07142830e+00]\n",
            "   [2.42704544e+01 1.32704554e+01 1.12704554e+01]\n",
            "   ...\n",
            "   [4.66529694e+01 1.92957611e+01 1.60814972e+01]\n",
            "   [4.95051727e+01 1.95051708e+01 1.75051708e+01]\n",
            "   [5.20154915e+01 2.10154915e+01 1.80154915e+01]]]\n",
            "\n",
            "\n",
            " [[[1.95387756e+02 1.97387756e+02 1.58387756e+02]\n",
            "   [1.97214279e+02 1.99214279e+02 1.62214279e+02]\n",
            "   [2.06214294e+02 2.07642868e+02 1.76285721e+02]\n",
            "   ...\n",
            "   [1.87142792e+02 1.81280533e+02 1.58719299e+02]\n",
            "   [1.94515472e+02 1.89872620e+02 1.66801193e+02]\n",
            "   [2.13178406e+02 2.08178406e+02 1.87464127e+02]]\n",
            "\n",
            "  [[1.61719391e+02 1.63719391e+02 1.24719391e+02]\n",
            "   [1.85158157e+02 1.86295914e+02 1.51882660e+02]\n",
            "   [1.92239792e+02 1.93224487e+02 1.61484695e+02]\n",
            "   ...\n",
            "   [1.51433777e+02 1.48520477e+02 1.33234772e+02]\n",
            "   [1.54301010e+02 1.51505112e+02 1.34153061e+02]\n",
            "   [1.63740005e+02 1.61668579e+02 1.46382858e+02]]\n",
            "\n",
            "  [[9.38775558e+01 9.48775558e+01 6.13061218e+01]\n",
            "   [1.25581635e+02 1.26581635e+02 9.30102081e+01]\n",
            "   [1.40204086e+02 1.41204086e+02 1.09418365e+02]\n",
            "   ...\n",
            "   [8.85614319e+01 8.89899826e+01 8.20613708e+01]\n",
            "   [8.97245255e+01 9.09388123e+01 8.35816727e+01]\n",
            "   [9.68111191e+01 9.88162994e+01 9.11019821e+01]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[1.37806442e+02 6.65206833e+01 1.10920887e+01]\n",
            "   [1.45025818e+02 7.24543457e+01 1.76686115e+01]\n",
            "   [1.44928970e+02 7.01891174e+01 1.62145920e+01]\n",
            "   ...\n",
            "   [6.43367081e+01 3.04285278e+01 3.85705566e+00]\n",
            "   [7.46887131e+01 3.86887131e+01 1.66887131e+01]\n",
            "   [7.32857666e+01 3.52857666e+01 1.62857666e+01]]\n",
            "\n",
            "  [[1.49790817e+02 7.58622742e+01 1.05765057e+01]\n",
            "   [1.64428574e+02 8.94948959e+01 2.42805748e+01]\n",
            "   [1.56658096e+02 8.14591217e+01 1.68570385e+01]\n",
            "   ...\n",
            "   [7.16735916e+01 3.68878517e+01 6.11748600e+00]\n",
            "   [7.93927536e+01 4.33927536e+01 1.93927517e+01]\n",
            "   [7.56635666e+01 3.86635666e+01 1.96635647e+01]]\n",
            "\n",
            "  [[1.60020477e+02 8.60204697e+01 1.23061171e+01]\n",
            "   [1.67556107e+02 9.35560989e+01 1.98417454e+01]\n",
            "   [1.64653320e+02 8.85768204e+01 1.57961082e+01]\n",
            "   ...\n",
            "   [6.66784134e+01 3.18926754e+01 2.29590893e-01]\n",
            "   [7.11990891e+01 3.51990891e+01 9.34197235e+00]\n",
            "   [8.79184036e+01 5.09184074e+01 3.19184074e+01]]]\n",
            "\n",
            "\n",
            " [[[7.87244892e+00 2.87244892e+00 6.87244892e+00]\n",
            "   [6.59693909e+00 4.59693909e+00 7.59693909e+00]\n",
            "   [6.00000000e+00 4.00000000e+00 7.00000000e+00]\n",
            "   ...\n",
            "   [5.57654476e+00 9.79080868e+00 1.12193365e+01]\n",
            "   [1.17857275e+01 7.64284420e+00 8.57140255e+00]\n",
            "   [1.50714283e+01 6.35714293e+00 4.71428585e+00]]\n",
            "\n",
            "  [[8.00000000e+00 4.00000000e+00 5.00000000e+00]\n",
            "   [6.99489784e+00 4.99489784e+00 5.99489784e+00]\n",
            "   [6.72959185e+00 4.72959185e+00 5.72959185e+00]\n",
            "   ...\n",
            "   [4.96939087e+00 5.08673286e+00 7.07142830e+00]\n",
            "   [6.21940136e+00 6.00000000e+00 5.92854500e+00]\n",
            "   [7.23979759e+00 5.97448730e+00 4.07142830e+00]]\n",
            "\n",
            "  [[7.00000000e+00 5.21428585e+00 5.57142830e+00]\n",
            "   [7.92857170e+00 6.14285755e+00 6.50000000e+00]\n",
            "   [6.00000000e+00 6.00000000e+00 5.57142830e+00]\n",
            "   ...\n",
            "   [1.28060265e+01 4.18881416e+00 6.61734200e+00]\n",
            "   [5.88263178e+00 6.05613279e+00 5.98469114e+00]\n",
            "   [2.21425056e+00 7.35717773e+00 5.78571415e+00]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[1.55362244e+02 1.38147980e+02 1.18147972e+02]\n",
            "   [1.58729614e+02 1.41515350e+02 1.21515350e+02]\n",
            "   [1.57214264e+02 1.40877563e+02 1.18382614e+02]\n",
            "   ...\n",
            "   [2.52617386e+02 2.49260178e+02 2.20045914e+02]\n",
            "   [2.53642914e+02 2.50285706e+02 2.21071442e+02]\n",
            "   [2.53214294e+02 2.49857086e+02 2.20642822e+02]]\n",
            "\n",
            "  [[1.59071442e+02 1.44071442e+02 1.23071442e+02]\n",
            "   [1.58071426e+02 1.43071426e+02 1.20071426e+02]\n",
            "   [1.57229584e+02 1.42229584e+02 1.19229599e+02]\n",
            "   ...\n",
            "   [2.51198959e+02 2.47198959e+02 2.18627487e+02]\n",
            "   [2.53071442e+02 2.49071442e+02 2.20071442e+02]\n",
            "   [2.52974487e+02 2.48974487e+02 2.19974487e+02]]\n",
            "\n",
            "  [[1.60229645e+02 1.48229645e+02 1.24229652e+02]\n",
            "   [1.60831802e+02 1.48831802e+02 1.24831802e+02]\n",
            "   [1.60275665e+02 1.47632812e+02 1.23847099e+02]\n",
            "   ...\n",
            "   [2.50000000e+02 2.46000000e+02 2.19000000e+02]\n",
            "   [2.52668335e+02 2.48668335e+02 2.19668335e+02]\n",
            "   [2.52642822e+02 2.48642822e+02 2.19642822e+02]]]], shape=(32, 224, 224, 3), dtype=float32) tf.Tensor(\n",
            "[[0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]], shape=(32, 10), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 0: Bulding a transfer learning feature extraction model using the Keras Functional API\n",
        "\n",
        "The sequential API is straight-forward, it runs our layers in sequential order.\n",
        "But the functional API gives us more flexibility with our models.\n",
        "\n"
      ],
      "metadata": {
        "id": "iUStfg7bQhV9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Create base model with tf.keras.application\n",
        "base_model = tf.keras.applications.EfficientNetB0(include_top=False)\n",
        "\n",
        "# 2. Freeze the base model (so the underlying pre-trained patterns aren't update)\n",
        "base_model.trainable = False\n",
        "\n",
        "# 3. Create inputs into our model\n",
        "inputs = tf.keras.layers.Input(shape=(224, 224, 3), name=\"input_layer\")\n",
        "\n",
        "# 4. If using ResNet50V2 you will need to normalize inputs\n",
        "# x = tf.keras.layers.experimental.Rescaling(1/255.)(inputs)\n",
        "\n",
        "# 5. Pass the inputs to the base_model\n",
        "x = base_model(inputs)\n",
        "print(f\"Shape after passing input through base model: {x.shape}\")\n",
        "\n",
        "# 6. Average pool the outputs of the base model (aggregate all the mose important information, reduce number of computations)\n",
        "x = tf.keras.layers.GlobalAveragePooling2D(name=\"global_average_pooling_layer\")(x)\n",
        "print(f\"Shape after GlobalAveragePooling2D: {x.shape}\")\n",
        "\n",
        "# 7. Create the output activation layer\n",
        "outputs = tf.keras.layers.Dense(10, activation=\"softmax\", name=\"output_layer\")(x)\n",
        "\n",
        "# 8. Combine the inputs with the outputs into a model\n",
        "model_0 = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "# 9. Compile the model\n",
        "model_0.compile(loss=\"categorical_crossentropy\",\n",
        "                optimizer=tf.keras.optimizers.Adam(),\n",
        "                metrics=[\"accuracy\"])\n",
        "\n",
        "# 10. Fit the model and save its history\n",
        "history_10_percent = model_0.fit(train_data_10_percent,\n",
        "                                 epochs=5,\n",
        "                                 steps_per_epoch=len(train_data_10_percent),\n",
        "                                 validation_data=test_data,\n",
        "                                 validation_steps=int(0.25 * len(test_data)),\n",
        "                                 callbacks=[create_tensorboard_callback(dir_name=\"transfer_learning\",\n",
        "                                                                        experiment_name=\"10_percent_feature_extraction\")])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YneyphOSpaka",
        "outputId": "be1b2414-d042-419c-eeb3-f0bc4e3c0cea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb0_notop.h5\n",
            "16705208/16705208 [==============================] - 0s 0us/step\n",
            "Shape after passing input through base model: (None, 7, 7, 1280)\n",
            "Shape after GlobalAveragePooling2D: (None, 1280)\n",
            "Saving TensorBoard log files to: transfer_learning/10_percent_feature_extraction/20230927-173553\n",
            "Epoch 1/5\n",
            "24/24 [==============================] - 147s 6s/step - loss: 1.9306 - accuracy: 0.3707 - val_loss: 1.3368 - val_accuracy: 0.7039\n",
            "Epoch 2/5\n",
            "24/24 [==============================] - 136s 6s/step - loss: 1.1251 - accuracy: 0.7680 - val_loss: 0.8873 - val_accuracy: 0.8076\n",
            "Epoch 3/5\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.7903 - accuracy: 0.8387"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate on the full test dataset\n",
        "model_0.evaluate(test_data)"
      ],
      "metadata": {
        "id": "VI39IKz_srtp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the layers in our base model\n",
        "for layer_number, layer in enumerate(base_model.layers):\n",
        "  print(layer_number, layer.name)"
      ],
      "metadata": {
        "id": "ksZ4s_RStKCq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# How about we get a summary of the base model?\n",
        "base_model.summary()"
      ],
      "metadata": {
        "id": "R-PXVnWRtXAJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# How about a summary of our whole model?\n",
        "model_0.summary()"
      ],
      "metadata": {
        "id": "XUMI1dHNtn2y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check out our model's training curves\n",
        "plot_loss_curves(history_10_percent)"
      ],
      "metadata": {
        "id": "F8fo3iE0t4Ip"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Getting a feature vector from a trained model\n",
        "\n",
        "Let's demonstrate the GlobalAveragePooling2D layer...\n",
        "\n",
        "We have a tensor after our model goes through `base_model` of shape (None, 7, 7, 128)...\n",
        "\n",
        "But then when it passes through GlobalAveragePooling2D, it turns into (None, 1280).\n",
        "\n",
        "Let's use a similar shaped tensor of (1, 4, 4, 3) and then pass it to GlobalAveragePooling2D."
      ],
      "metadata": {
        "id": "cYqjJkxBuKIb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the input shape\n",
        "input_shape = (1, 4, 4, 3)\n",
        "\n",
        "# Create a random tensor\n",
        "tf.random.set_seed(42)\n",
        "input_tensor = tf.random.normal(input_shape)\n",
        "print(f\"Random input tensor:\\n {input_tensor}\\n\")\n",
        "\n",
        "# Pass the random tenosr through the global average pooling 2D layer\n",
        "global_average_pooled_tensor = tf.keras.layers.GlobalAveragePooling2D()(input_tensor)\n",
        "print(f\"2D global average pooled random tensor:\\n {global_average_pooled_tensor}\\n\")\n",
        "\n",
        "# Check the shape of different tensors\n",
        "print(f\"Shape of input tensor: {input_tensor.shape}\")\n",
        "print(f\"Shape of Global Average Pooled 2D tensor: {global_average_pooled_tensor.shape}\")"
      ],
      "metadata": {
        "id": "Bv1DRL0JuvK6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's replicate the GloabalAveragePool2D layer\n",
        "tf.reduce_mean(input_tensor, axis=[1, 2])"
      ],
      "metadata": {
        "id": "AgQkU_v4zmia"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Running a series of transfer learning experiments\n",
        "\n",
        "We've seen the incredible results transfer learning can get with only 10% of the traning data, but how does it go with 1% of the traning data... how about we setup a bunch of experiment to find out:\n",
        "1. `model_1` - use feature extraction transfer learning with 1% of the training data with data augmentation\n",
        "2. `model_2` - use feature extraction transfer learning with 10% of the training with data augmentation\n",
        "3. `model_3` - use fine-tuning transfer learning on 10% of the training data with data augmentation\n",
        "4. `model_4` - use fine-tuning transfer learning on 100% of the training data with augmentation\n",
        "\n",
        "Note: throughout all experiments the same test dataset will be used to evaluate our model... this ensure consistency across evaluation metrics."
      ],
      "metadata": {
        "id": "uWu1DS7v2dyx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Getting and preprocessing data for model_1"
      ],
      "metadata": {
        "id": "xtnd1C5b-exC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download and unzip data - preprocess from Food101\n",
        "!wget https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_1_percent.zip\n",
        "\n",
        "unzip_data(\"10_food_classes_1_percent.zip\")"
      ],
      "metadata": {
        "id": "7_Jnt0ok9IhU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create training and test dirs\n",
        "train_dir_1_percent = \"10_food_classes_1_percent/train\"\n",
        "test_dir = \"10_food_classes_1_percent/test\""
      ],
      "metadata": {
        "id": "vSTy8zJG96_s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# How many images we are working with?\n",
        "walk_through_dir(\"10_food_classes_1_percent\")"
      ],
      "metadata": {
        "id": "M-p5A6hm-IT1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup data loaders\n",
        "IMG_SIZE = (224, 224)\n",
        "train_data_1_percent = tf.keras.utils.image_dataset_from_directory(train_dir_1_percent,\n",
        "                                                                   label_mode=\"categorical\",\n",
        "                                                                   image_size=IMG_SIZE,\n",
        "                                                                   batch_size=BATCH_SIZE)\n",
        "\n",
        "test_data = tf.keras.utils.image_dataset_from_directory(test_dir,\n",
        "                                                        label_mode=\"categorical\",\n",
        "                                                        image_size=IMG_SIZE,\n",
        "                                                        batch_size=BATCH_SIZE)"
      ],
      "metadata": {
        "id": "AMWs0DTM-Qpc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Adding data augmentaion right into the model\n",
        "\n",
        "To add data augmentation right into our models, we can use the layers inside:\n",
        "\n",
        "* `tf.keras.layers.experimental.preprocessing()`"
      ],
      "metadata": {
        "id": "VU5VxdmX_GiM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import preprocessing\n",
        "\n",
        "# Create data augmentation stage with horizontal flipping, rotations, zooms, etc\n",
        "data_augmentation = keras.Sequential([\n",
        "    layers.RandomFlip(\"horizontal\"),\n",
        "    layers.RandomRotation(0.2),\n",
        "    layers.RandomZoom(0.2),\n",
        "    layers.RandomHeight(0.2),\n",
        "    layers.RandomWidth(0.2),\n",
        "    # layers.Rescaling(1./255) # Keep for models like ResNet50V2 but EfficientNet's having rescaling builtin\n",
        "], name=\"data_augmentation\")"
      ],
      "metadata": {
        "id": "e9NG6_7SCnK2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualize our data augmentation layer (and see what happens to our data)"
      ],
      "metadata": {
        "id": "I0UwugNYC5A2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# View a random image and compare it to its augmented version\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import os\n",
        "import random\n",
        "\n",
        "target_class = random.choice(train_data_10_percent.class_names)\n",
        "target_dir = \"10_food_classes_1_percent/train/\" + target_class\n",
        "random_image = random.choice(os.listdir(target_dir))\n",
        "random_image_path = target_dir + \"/\" + random_image\n",
        "\n",
        "# Read in the random image\n",
        "img = mpimg.imread(random_image_path)\n",
        "plt.imshow(img)\n",
        "plt.title(f\"Original random image from class {target_class}\")\n",
        "plt.axis(False);\n",
        "\n",
        "# Now let's plot our augmented random iamge\n",
        "augmented_img = data_augmentation(img/255.)\n",
        "plt.figure()\n",
        "plt.imshow(augmented_img)\n",
        "plt.title(f\"Augmented random image from {target_class}\")\n",
        "plt.axis(False);"
      ],
      "metadata": {
        "id": "gGRRoeLYJTbX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -la /local/cache"
      ],
      "metadata": {
        "id": "T5IuIaCRJ8Hv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 1: Feature extraction transfer learning on 1% of the data with data augmentation"
      ],
      "metadata": {
        "id": "-Lf5VI82M0C_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup input sahpe and base model. freezing the base mdoel layers\n",
        "input_shape = IMG_SIZE + (3,)\n",
        "base_model = tf.keras.applications.EfficientNetB0(include_top=False)\n",
        "base_model.trainable = False\n",
        "\n",
        "# Create input layer\n",
        "inputs = layers.Input(shape=input_shape, name='input_layer')\n",
        "\n",
        "# Add in data augmentation Sequential model as a layer\n",
        "x = data_augmentation(inputs)\n",
        "\n",
        "# Give base_model the inputs (after augmenation) and don't train it\n",
        "x = base_model(x, training=False)\n",
        "\n",
        "# Pool output features of the base model\n",
        "x = layers.GlobalAveragePooling2D(name=\"global_average_pooling_layer\")(x)\n",
        "\n",
        "# Put a dense layer on as the output\n",
        "outputs = layers.Dense(10, activation=\"softmax\", name=\"output_layer\")(x)\n",
        "\n",
        "# Make a model using the inputs and outputs\n",
        "model_1 = keras.Model(inputs, outputs)\n",
        "\n",
        "# Compile the model\n",
        "model_1.compile(loss=\"categorical_crossentropy\",\n",
        "                optimizer=tf.keras.optimizers.Adam(),\n",
        "                metrics=[\"accuracy\"])\n",
        "\n",
        "# Fit the model\n",
        "history_1_percent = model_1.fit(train_data_1_percent,\n",
        "                                epochs=5,\n",
        "                                steps_per_epoch=len(train_data_1_percent),\n",
        "                                validation_data=test_data,\n",
        "                                validation_steps=int(0.25 * len(test_data)),\n",
        "                                callbacks=[create_tensorboard_callback(dir_name=\"transfer_learning\",\n",
        "                                                                       experiment_name=\"1_percent_data_aug\")])"
      ],
      "metadata": {
        "id": "bJdlejZPM9O2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check out a model summary\n",
        "model_1.summary()"
      ],
      "metadata": {
        "id": "BntrP6dlPUjY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate on the full test dataset\n",
        "model_1.evaluate(test_data)"
      ],
      "metadata": {
        "id": "icWL15vhQB8n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# How do the model with 1% of the data augmentation loss curves\n",
        "plot_loss_curves(history_1_percent)"
      ],
      "metadata": {
        "id": "L7iwcVabQLi_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 2: feature extraction transfer learning model with 10% of data and data augmentation"
      ],
      "metadata": {
        "id": "ZaIRuyZUQoNY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dir_10_percent = \"10_food_classes_10_percent/train\"\n",
        "test_dir = \"10_food_classes_10_percent/test\""
      ],
      "metadata": {
        "id": "uYp5JlSaRW6d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set data inputs\n",
        "import tensorflow as tf\n",
        "IMG_SIZE = (224, 224)\n",
        "train_data_10_percent = tf.keras.utils.image_dataset_from_directory(train_dir_10_percent,\n",
        "                                                                    label_mode=\"categorical\",\n",
        "                                                                    image_size=IMG_SIZE)\n",
        "\n",
        "test_data = tf.keras.utils.image_dataset_from_directory(test_dir,\n",
        "                                                        label_mode=\"categorical\",\n",
        "                                                        image_size=IMG_SIZE)"
      ],
      "metadata": {
        "id": "Y1tgxfL5Rh5r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create model 2 with data augmentation built in\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential\n",
        "\n",
        "# Build data augmentation layer\n",
        "data_augmentation = Sequential([\n",
        "    layers.RandomFlip(\"horizontal\"),\n",
        "    layers.RandomHeight(0.2),\n",
        "    layers.RandomWidth(0.2),\n",
        "    layers.RandomZoom(0.2),\n",
        "    layers.RandomRotation(0.2)\n",
        "    # layers.Rescaling(1./255)\n",
        "], name=\"data_augmentation\")\n",
        "\n",
        "# Setpu the input shep to ou model\n",
        "input_shape = (224, 224, 3)\n",
        "\n",
        "# Create a frozen base model (also called backbone)\n",
        "base_model = tf.keras.applications.EfficientNetB0(include_top=False)\n",
        "base_model.trainable = False\n",
        "\n",
        "# Create the niputs and outputs (including the layers between)\n",
        "inputs = layers.Input(shape=input_shape, name=\"input_layer\")\n",
        "x = data_augmentation(inputs)\n",
        "x = base_model(x, training=False) # pass augmented images to base model but keep the base model in inference mode, this also insurse batchnorm layers don't get updated\n",
        "x = layers.GlobalAveragePooling2D(name=\"global_average_pooling_2D\")(x)\n",
        "outputs = layers.Dense(10, activation=\"softmax\")(x)\n",
        "model_2 = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "# Compile\n",
        "model_2.compile(loss=\"categorical_crossentropy\",\n",
        "                optimizer=tf.keras.optimizers.Adam(),\n",
        "                metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "a-S8dPTeSCrw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_2.summary()"
      ],
      "metadata": {
        "id": "2692M8uOU63i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating a ModelCheckpoint callback\n",
        "\n",
        "The ModelCheckpoint callback intermediatley saves our model (the full model or just the weights) during training. This is usefull so we can come and start where we left off."
      ],
      "metadata": {
        "id": "oWHAj22kVHhu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set checkpoint path\n",
        "checkpoint_path = \"ten_percent_model_checkpoints_weights/checkpoint.ckpt\"\n",
        "\n",
        "# Create a ModelCheckpoint callback that saves the model's weights only\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
        "                                                         save_weights_only=True,\n",
        "                                                         save_best_only=False,\n",
        "                                                         save_freq=\"epoch\",\n",
        "                                                         verbose=1) # save every epoch"
      ],
      "metadata": {
        "id": "KhmGIA_CeiIo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fit model 2 passing in the ModelCheckpoint callback"
      ],
      "metadata": {
        "id": "fF-DWKUygd2a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the model saving checkpoints every epoch\n",
        "initial_epochs = 5\n",
        "history_10_percent_data_aug = model_2.fit(train_data_10_percent,\n",
        "                                          epochs=initial_epochs,\n",
        "                                          validation_data=test_data,\n",
        "                                          validation_steps=int(0.25 * len(test_data)),\n",
        "                                          callbacks=[create_tensorboard_callback(dir_name=\"transfer_learning\",\n",
        "                                                                                 experiment_name=\"10_percent_data_aug\"),\n",
        "                                                     checkpoint_callback])"
      ],
      "metadata": {
        "id": "pPkBO65_g2qI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_0.evaluate(test_data)"
      ],
      "metadata": {
        "id": "Es0AabehhbXx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_10_percent_data_aug = model_2.evaluate(test_data)\n",
        "results_10_percent_data_aug"
      ],
      "metadata": {
        "id": "kMBwEgjVh2XQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Plot model loss curves\n",
        "plot_loss_curves(history_10_percent_data_aug)"
      ],
      "metadata": {
        "id": "KPA6b3owh_Vx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading in checkpointed weights\n",
        "\n",
        "Loading ing checkpointed weights returns a model to specific checkpoint."
      ],
      "metadata": {
        "id": "zvJdTFwZiTL3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load in saved model weights and evaluate model\n",
        "model_2.load_weights(checkpoint_path)"
      ],
      "metadata": {
        "id": "NeNZYYvei6jA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate model_2 with loaded weights\n",
        "loaded_weights_model_resluts = model_2.evaluate(test_data)"
      ],
      "metadata": {
        "id": "axZQz3S_jRdp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# If the results from our previously evaluated model_2 match the loaded weights, everything has worked!\n",
        "results_10_percent_data_aug == loaded_weights_model_resluts"
      ],
      "metadata": {
        "id": "tqj-X92ZjZ6o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_10_percent_data_aug"
      ],
      "metadata": {
        "id": "JznksLaHjxrx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_weights_model_resluts"
      ],
      "metadata": {
        "id": "hMh9-nzAj0xZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checkto see if loaded model results are very close to our previous non loaded model results\n",
        "import numpy as np\n",
        "np.isclose(np.array(results_10_percent_data_aug), np.array(loaded_weights_model_resluts))"
      ],
      "metadata": {
        "id": "rOcI-90hj2Qw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the differnce between results the two results\n",
        "print(np.array(results_10_percent_data_aug) - np.array(loaded_weights_model_resluts))"
      ],
      "metadata": {
        "id": "ARMKTQnckLsR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 3: Fine tuning an existing model on 10% of data\n",
        "\n",
        "> **Note:** Fine-tuning usually works best *after* training a feature extraction model for a few epochs with large amounts of custom data."
      ],
      "metadata": {
        "id": "3Q8M8YUnkayR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Layers in loaded model\n",
        "model_2.layers"
      ],
      "metadata": {
        "id": "SOK8NQwFlaY5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Are these layers trainable?\n",
        "for layer in model_2.layers:\n",
        "  print(layer, layer.trainable)"
      ],
      "metadata": {
        "id": "kHIJYPhumqzR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# What layers are in our base_model(EfficientNetB0) and are the trainable?\n",
        "for i, layer in enumerate(model_2.layers[2].layers):\n",
        "  print(i, layer.name, layer.trainable)"
      ],
      "metadata": {
        "id": "k84lTNc-myK5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# How many trainable variables are in our base model?\n",
        "print(len(model_2.layers[2].trainable_variables))"
      ],
      "metadata": {
        "id": "djXtiJRPnGCJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To begin fine tuning let's start by setting the last 10 layers of our base_model.trainable=True\n",
        "base_model.trainable = True\n",
        "\n",
        "# Freeze all layers except for the last 10\n",
        "for layer in base_model.layers[:-10]:\n",
        "  layer.trainable = False\n",
        "\n",
        "# Recompile (we have to recompile our models ever time we make a change)\n",
        "model_2.compile(loss=\"categorical_crossentropy\",\n",
        "                optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), # when fine tuning you typically want to lower the learning rate by 10x\n",
        "                metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "qEhiZlrMn1WB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Note:** When using fine-tuning it's best practive to lower your learning rate by some amount. How much? This is a hyperparameter you can tune. But a good rule of thumb is at least 10x (though different sources will claim other values)."
      ],
      "metadata": {
        "id": "foVf0UzrqwG7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check which layers are tunable (trainable)\n",
        "for layer_number, layer in enumerate(model_2.layers[2].layers):\n",
        "  print(layer_number, layer.name, layer.trainable)"
      ],
      "metadata": {
        "id": "U4ekoeSoo74R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now we've unfrozen some of the layers closer to the top, how many trainable varibales are there?\n",
        "print(len(model_2.trainable_variables))"
      ],
      "metadata": {
        "id": "zPgTflIYpuZC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_2.trainable_variables"
      ],
      "metadata": {
        "id": "VV1BNWxTr8Ap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fine tune for another 5 epochs\n",
        "fine_tune_epochs = initial_epochs + 5\n",
        "\n",
        "# Refit the model (same as model_2 except with more trainable layers)\n",
        "history_fine_10_percent_data_aug = model_2.fit(train_data_10_percent,\n",
        "                                               epochs=fine_tune_epochs,\n",
        "                                               validation_data=test_data,\n",
        "                                               validation_steps=int(0.25 * len(test_data)),\n",
        "                                               initial_epoch=history_10_percent_data_aug.epoch[-1], # start training from previous las epoch\n",
        "                                               callbacks=[create_tensorboard_callback(dir_name=\"transfer_learning\",\n",
        "                                                                                      experiment_name=\"10_percent_fine_tune_last_10\")])"
      ],
      "metadata": {
        "id": "pOhNyFnEr_Xx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the fin tuned model (model_3 which is actualy model_2 fine_tuned for another 5)\n",
        "results_fine_tuned_10_percent = model_2.evaluate(test_data)"
      ],
      "metadata": {
        "id": "dSVEWNIWul2N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_10_percent_data_aug"
      ],
      "metadata": {
        "id": "lC2vlUmLu4JR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FQK-P9Dpu6WB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}