{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to NLP Fundamentals in TensorFlow\n",
    "\n",
    "NLP has the goal of deriving information out of natural language (could be sequences text or speech).\n",
    "\n",
    "Another common term for NLP problems is sequence to sequence problems (seq2seq)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Oct 11 19:05:39 2023       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 520.56.06    Driver Version: 522.30       CUDA Version: 11.8     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  NVIDIA GeForce ...  On   | 00000000:01:00.0  On |                  N/A |\r\n",
      "|  0%   44C    P0    48W / 170W |    543MiB / 12288MiB |      2%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|  No running processes found                                                 |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-11 19:05:42.730752: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Import a series fo helper functions for the notebok\n",
    "from helper_functions import unzip_data, create_tensorboard_callback, plot_loss_curves, compare_historys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get a text dataset\n",
    "\n",
    "The dataset we're going to be using is Kaggle's introduction to NLP dataset (text samples of Tweets labelled as disaster or not disaster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize a text dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "assets_dir = \"assets\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "train_df = pd.read_csv(os.path.join(assets_dir, \"train.csv\"))\n",
    "test_df = pd.read_csv(os.path.join(assets_dir, \"test.csv\"))\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Forest fire near La Ronge Sask. Canada'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[\"text\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2644</th>\n",
       "      <td>3796</td>\n",
       "      <td>destruction</td>\n",
       "      <td>NaN</td>\n",
       "      <td>So you have a new weapon that can cause un-ima...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2227</th>\n",
       "      <td>3185</td>\n",
       "      <td>deluge</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The f$&amp;amp;@ing things I do for #GISHWHES Just...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5448</th>\n",
       "      <td>7769</td>\n",
       "      <td>police</td>\n",
       "      <td>UK</td>\n",
       "      <td>DT @georgegalloway: RT @Galloway4Mayor: ÛÏThe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>191</td>\n",
       "      <td>aftershock</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Aftershock back to school kick off was great. ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6845</th>\n",
       "      <td>9810</td>\n",
       "      <td>trauma</td>\n",
       "      <td>Montgomery County, MD</td>\n",
       "      <td>in response to trauma Children of Addicts deve...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id      keyword               location  \\\n",
       "2644  3796  destruction                    NaN   \n",
       "2227  3185       deluge                    NaN   \n",
       "5448  7769       police                     UK   \n",
       "132    191   aftershock                    NaN   \n",
       "6845  9810       trauma  Montgomery County, MD   \n",
       "\n",
       "                                                   text  target  \n",
       "2644  So you have a new weapon that can cause un-ima...       1  \n",
       "2227  The f$&amp;@ing things I do for #GISHWHES Just...       0  \n",
       "5448  DT @georgegalloway: RT @Galloway4Mayor: ÛÏThe...       1  \n",
       "132   Aftershock back to school kick off was great. ...       0  \n",
       "6845  in response to trauma Children of Addicts deve...       0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shuffle train dataframe\n",
    "train_df_shuffled = train_df.sample(frac=1, random_state=42)\n",
    "train_df_shuffled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text\n",
       "0   0     NaN      NaN                 Just happened a terrible car crash\n",
       "1   2     NaN      NaN  Heard about #earthquake is different cities, s...\n",
       "2   3     NaN      NaN  there is a forest fire at spot pond, geese are...\n",
       "3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires\n",
       "4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What does the test dataframe look like?\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "0    4342\n",
       "1    3271\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many examples of each class?\n",
    "train_df.target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7613, 3263)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many total samples?\n",
    "len(train_df), len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target: 0 (not real disaster)\n",
      "Text:\n",
      "They are the real heroes... RIP Brave hearts... http://t.co/Q9LxO4QkjI\n",
      "\n",
      "---\n",
      "\n",
      "Target: 1 (real disaster)\n",
      "Text:\n",
      "16yr old PKK suicide bomber who detonated bomb in Turkey Army trench released http://t.co/mMkLapX2ok\n",
      "\n",
      "---\n",
      "\n",
      "Target: 0 (not real disaster)\n",
      "Text:\n",
      "http://t.co/X5XUMtoEkE Nuclear Emergency Current USA radiation levels monitoring site\n",
      "\n",
      "---\n",
      "\n",
      "Target: 1 (real disaster)\n",
      "Text:\n",
      "16 dead in Russia bus accident: At least 16 people were killed and 26 others injured when two buses collided i... http://t.co/ybyP68ieVn\n",
      "\n",
      "---\n",
      "\n",
      "Target: 1 (real disaster)\n",
      "Text:\n",
      "#BHRAMABULL Watch Run The Jewels Use Facts to Defend Rioting in Ferguson: The socially minded duo takes on the... http://t.co/Ld5P1sIa2N\n",
      "\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's visualize some random training examples\n",
    "import random\n",
    "random_index = random.randint(0, len(train_df) - 5)\n",
    "for row in train_df_shuffled[[\"text\", \"target\"]][random_index:random_index+5].itertuples():\n",
    "    _, text, target = row\n",
    "    print(f\"Target: {target}\", \"(real disaster)\" if target > 0 else \"(not real disaster)\")\n",
    "    print(f\"Text:\\n{text}\\n\")\n",
    "    print(\"---\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into trainig and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use train_test_split to split training data inot training and validation sets\n",
    "train_sentences, val_sentences, train_labels, val_labels = train_test_split(train_df_shuffled[\"text\"].to_numpy(),\n",
    "                                                                           train_df_shuffled[\"target\"].to_numpy(),\n",
    "                                                                           test_size=0.1,\n",
    "                                                                           random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6851, 6851, 762, 762)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the lengths\n",
    "len(train_sentences), len(train_labels), len(val_sentences), len(val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['@mogacola @zamtriossu i screamed after hitting tweet',\n",
       "        'Imagine getting flattened by Kurt Zouma',\n",
       "        '@Gurmeetramrahim #MSGDoing111WelfareWorks Green S welfare force ke appx 65000 members har time disaster victim ki help ke liye tyar hai....',\n",
       "        \"@shakjn @C7 @Magnums im shaking in fear he's gonna hack the planet\",\n",
       "        'Somehow find you and I collide http://t.co/Ee8RpOahPk',\n",
       "        '@EvaHanderek @MarleyKnysh great times until the bus driver held us hostage in the mall parking lot lmfao',\n",
       "        'destroy the free fandom honestly',\n",
       "        'Weapons stolen from National Guard Armory in New Albany still missing #Gunsense http://t.co/lKNU8902JE',\n",
       "        '@wfaaweather Pete when will the heat wave pass? Is it really going to be mid month? Frisco Boy Scouts have a canoe trip in Okla.',\n",
       "        'Patient-reported outcomes in long-term survivors of metastatic colorectal cancer - British Journal of Surgery http://t.co/5Yl4DC1Tqt'],\n",
       "       dtype=object),\n",
       " array([0, 0, 1, 0, 0, 1, 1, 0, 1, 1]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the first 10 samples\n",
    "train_sentences[:10], train_labels[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting texts to numbers\n",
    "\n",
    "When dealing with a text problem, one of the first things you'll have to do before you can build a mdoel ist to convert your text to numbers.\n",
    "\n",
    "There are a few ways to do this, namely:\n",
    "* Tokenaization - direct mapping of token (a token could be a word or a character) to number\n",
    "* Embedding - create a matrix of feature vector for each token (the size of the feature vector can be defined and this embedding can be learned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text vectroization (tokenization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['@mogacola @zamtriossu i screamed after hitting tweet',\n",
       "       'Imagine getting flattened by Kurt Zouma',\n",
       "       '@Gurmeetramrahim #MSGDoing111WelfareWorks Green S welfare force ke appx 65000 members har time disaster victim ki help ke liye tyar hai....',\n",
       "       \"@shakjn @C7 @Magnums im shaking in fear he's gonna hack the planet\",\n",
       "       'Somehow find you and I collide http://t.co/Ee8RpOahPk'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sentences[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-11 19:05:53.282888: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-10-11 19:05:53.349020: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-10-11 19:05:53.349361: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-10-11 19:05:53.351640: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-10-11 19:05:53.352045: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-10-11 19:05:53.352309: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-10-11 19:05:55.587387: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-10-11 19:05:55.587935: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-10-11 19:05:55.587954: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1726] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2023-10-11 19:05:55.588312: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-10-11 19:05:55.588395: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9554 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "# Use the default TextVectorization parameters\n",
    "text_vectorizer = TextVectorization(max_tokens=None, # how many words in the vocabulary (automatically add <OVV>)\n",
    "                                    standardize=\"lower_and_strip_punctuation\",\n",
    "                                    split=\"whitespace\",\n",
    "                                    ngrams=None, # create groups of n-words\n",
    "                                    output_mode=\"int\", # how to map tokens to numbers\n",
    "                                    output_sequence_length=None, # how long do you want your sequences to be\n",
    "                                    pad_to_max_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the averagte number of tokens (words) in the training tweets\n",
    "round(sum([len(i.split()) for i in train_sentences])/len(train_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup text vectorization variables\n",
    "max_vocab_length = 1000 # max number of words to have in our vocabulary\n",
    "max_length = 15 # max length our sequences will be (e.g. how many words from a Tweet does a model see?)\n",
    " \n",
    "text_vectorizer = TextVectorization(max_tokens=max_vocab_length,\n",
    "                                   output_mode=\"int\",\n",
    "                                   output_sequence_length=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the text vectorizer to training text\n",
    "text_vectorizer.adapt(train_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 15), dtype=int64, numpy=\n",
       "array([[264,   3, 232,   4,  13, 698,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0]])>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a sample sentence and tokenize it\n",
    "sample_sentence = \"There's a flood in my street!\"\n",
    "text_vectorizer([sample_sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      " ouvindo Peace Love &amp; Armageddon        \n",
      "\n",
      "Vectorized version: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 15), dtype=int64, numpy=\n",
       "array([[  1, 675, 110,  35, 443,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0]])>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Choose random sentence from the training dataset and tokenize it\n",
    "random_sentence = random.choice(train_sentences)\n",
    "print(f\"Original text:\\n {random_sentence}\\\n",
    "        \\n\\nVectorized version: \")\n",
    "text_vectorizer([random_sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in vocab: 1000\n",
      "5 most common words: ['', '[UNK]', 'the', 'a', 'in']\n",
      "5 least common words: ['reported', 'r', 'pray', 'playlist', 'patience']\n"
     ]
    }
   ],
   "source": [
    "# Get the unique words in the vocabulary\n",
    "words_in_vocab = text_vectorizer.get_vocabulary() # get all of the unique words in our training data\n",
    "top_5_words = words_in_vocab[:5] # get the most common words\n",
    "bottom_5_words = words_in_vocab[-5:] # get the least common words\n",
    "print(f\"Number of words in vocab: {len(words_in_vocab)}\")\n",
    "print(f\"5 most common words: {top_5_words}\")\n",
    "print(f\"5 least common words: {bottom_5_words}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating an Embedding using Embedding Layer\n",
    "\n",
    "To make our embedding, we're going to user TensorFlows embedding layer\n",
    "\n",
    "The parameters we care most about for our embedding layer:\n",
    "* `input_dim` = the size of our vocabulary\n",
    "* `output_dim` = the size of the output embedding vector, for example, a value of 100 would mean each token gets represented by a vector 100 long\n",
    "* `input_length` = length of the sequences being passed to the embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.src.layers.core.embedding.Embedding at 0x7efec0080820>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "embedding = layers.Embedding(input_dim=max_vocab_length, # set input shape\n",
    "                             output_dim=128, # ouput shape\n",
    "                             input_length=max_length, # how long is each input\n",
    "                            )\n",
    "embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      " Turkish newlyweds donate wedding money - what a beautiful gesture! Still have faith in humanity. http://t.co/o1eNHjrkJd      \n",
      "\n",
      "Embedded version:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 15, 128), dtype=float32, numpy=\n",
       "array([[[ 0.03682767,  0.01449132,  0.02805474, ...,  0.00291402,\n",
       "         -0.00503036, -0.02739042],\n",
       "        [ 0.03682767,  0.01449132,  0.02805474, ...,  0.00291402,\n",
       "         -0.00503036, -0.02739042],\n",
       "        [ 0.03682767,  0.01449132,  0.02805474, ...,  0.00291402,\n",
       "         -0.00503036, -0.02739042],\n",
       "        ...,\n",
       "        [-0.01644727, -0.0200176 , -0.04677739, ..., -0.02471337,\n",
       "         -0.03889005,  0.04703153],\n",
       "        [ 0.03682767,  0.01449132,  0.02805474, ...,  0.00291402,\n",
       "         -0.00503036, -0.02739042],\n",
       "        [ 0.03682767,  0.01449132,  0.02805474, ...,  0.00291402,\n",
       "         -0.00503036, -0.02739042]]], dtype=float32)>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get a random sentence from the training set\n",
    "random_sentence = random.choice(train_sentences)\n",
    "print(f\"Original text:\\n {random_sentence}\\\n",
    "      \\n\\nEmbedded version:\")\n",
    "\n",
    "# Embed random sentence (turn it into dense vectors of fixed size)\n",
    "sample_embed = embedding(text_vectorizer([random_sentence]))\n",
    "sample_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(128,), dtype=float32, numpy=\n",
       " array([ 0.03682767,  0.01449132,  0.02805474, -0.01808115, -0.04755772,\n",
       "        -0.04103296, -0.04971376, -0.01422752,  0.03888191,  0.02277316,\n",
       "        -0.02845833,  0.02770403,  0.04861397, -0.02555586, -0.00025392,\n",
       "         0.03410666, -0.03558301, -0.03410198, -0.01246419,  0.02326603,\n",
       "         0.03231182, -0.04752398, -0.03127208,  0.00055163,  0.01985813,\n",
       "         0.03947299, -0.04250422, -0.04514734,  0.01324764,  0.00540216,\n",
       "        -0.04348728, -0.0421056 ,  0.00538255,  0.04285108, -0.0228676 ,\n",
       "        -0.03417899, -0.03376715,  0.04435996, -0.01065499,  0.00712367,\n",
       "        -0.00454094,  0.04231148, -0.00421479, -0.00077788,  0.03378535,\n",
       "         0.00807086, -0.03544935, -0.02581085, -0.01413932,  0.0225469 ,\n",
       "        -0.03336375, -0.03566853, -0.00827146,  0.04602125,  0.01311715,\n",
       "        -0.00119644,  0.03207025,  0.00825157,  0.04982836,  0.0251401 ,\n",
       "         0.03833194, -0.01930432,  0.03946593,  0.04277176,  0.03640184,\n",
       "        -0.02096665,  0.03407865,  0.03995859,  0.04071197,  0.02441784,\n",
       "        -0.02644298,  0.03021774, -0.03043919,  0.00942113,  0.02254436,\n",
       "        -0.02822918, -0.00906213,  0.03283068,  0.01603646,  0.01431967,\n",
       "        -0.00256778,  0.0165344 ,  0.02081292,  0.04264892,  0.01675612,\n",
       "         0.01684384,  0.03732778, -0.04055898,  0.03354486,  0.02307062,\n",
       "         0.01314825, -0.04766307,  0.03611984,  0.00838133, -0.03873546,\n",
       "         0.04541042, -0.03869456,  0.00955993,  0.011359  , -0.02346775,\n",
       "         0.01120987,  0.02324455,  0.00390798, -0.00962603,  0.01100075,\n",
       "        -0.0303988 , -0.01650946,  0.01596074,  0.0107474 , -0.04474069,\n",
       "        -0.0408495 ,  0.03863606,  0.02523842,  0.01125802,  0.00837792,\n",
       "        -0.00155214, -0.03147676, -0.00032548, -0.0149365 , -0.04775986,\n",
       "         0.02897158,  0.00277611,  0.02783123,  0.02445367, -0.02487429,\n",
       "         0.00291402, -0.00503036, -0.02739042], dtype=float32)>,\n",
       " TensorShape([128]),\n",
       " 'Turkish newlyweds donate wedding money - what a beautiful gesture! Still have faith in humanity. http://t.co/o1eNHjrkJd')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check out a single token's embedding\n",
    "sample_embed[0][0], sample_embed[0][0].shape, random_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling a text dataset\n",
    "\n",
    "\n",
    "Now that we've got a way to turn our text data into numbers, we can start to build machine learning models to model it.\n",
    "\n",
    "To get plenty of practice, we're going to build a series of different models, each as its own experiment. We'll then compare the results of each model and see which one performed best.\n",
    "\n",
    "More specifically, we'll be building the following:\n",
    "* **Model 0**: Naive Bayes (baseline)\n",
    "* **Model 1**: Feed-forward neural network (dense model)\n",
    "* **Model 2**: LSTM model\n",
    "* **Model 3**: GRU model\n",
    "* **Model 4**: Bidirectional-LSTM model\n",
    "* **Model 5**: 1D Convolutional Neural Network\n",
    "* **Model 6**: TensorFlow Hub Pretrained Feature Extractor\n",
    "* **Model 7**: Same as model 6 with 10% of training data\n",
    "\n",
    "Model 0 is the simplest to acquire a baseline which we'll expect each other of the other deeper models to beat.\n",
    "\n",
    "Each experiment will go through the following steps:\n",
    "* Construct the model\n",
    "* Train the model\n",
    "* Make predictions with the model\n",
    "* Track prediction evaluation metrics for later comparison\n",
    "\n",
    "Let's get started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 0: Getting a baseline\n",
    "\n",
    "As with all machine learning modelling experiments, it's important to create a baseline model so you've got a benchmark for future experiments to build upon.\n",
    "\n",
    "To create our baseline, we'll use Sklearn's Multinomial Naive Bayes using the TF-IDF formula to convert our words to numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;tfidf&#x27;, TfidfVectorizer()), (&#x27;clf&#x27;, MultinomialNB())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;tfidf&#x27;, TfidfVectorizer()), (&#x27;clf&#x27;, MultinomialNB())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('tfidf', TfidfVectorizer()), ('clf', MultinomialNB())])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Create tokenization and modelling pipeline\n",
    "model_0 = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer()), # convert words to numbers using tfidf\n",
    "    (\"clf\", MultinomialNB()) # model the text\n",
    "])\n",
    "\n",
    "# Fit the pipeline to the training data\n",
    "model_0.fit(train_sentences, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our baseline model achieves an accuracy of: 79.27%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate our baseline model\n",
    "baseline_score = model_0.score(val_sentences, val_labels)\n",
    "print(f\"Our baseline model achieves an accuracy of: {baseline_score*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make predictions\n",
    "baseline_preds = model_0.predict(val_sentences)\n",
    "baseline_preds[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating an evalutation function for our model experiments\n",
    "\n",
    "We could evaluate all of our model's predictions with different metrics every time, howeve, this will be cumbersome and could easily be fixed with a function\n",
    "\n",
    "Let's create on to compare our model's predictions with the truth labels using the following metrics:\n",
    "\n",
    "* Accuracy\n",
    "* Precision\n",
    "* Recall\n",
    "* F1-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluat: accuracy, precision, recall, f1-socre\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "def calculate_results(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculates model accuracy, precision, recall and f1 score of binary classification model.\n",
    "    \"\"\"\n",
    "    # Calculate model accuracy\n",
    "    model_accuracy = accuracy_score(y_true, y_pred) * 100\n",
    "    \n",
    "    # calculate model precision, recall and f1-score using \"weighted\" average\n",
    "    model_precision, model_recall, model_f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"weighted\")\n",
    "    model_results = {\"accuracy\": model_accuracy,\n",
    "                     \"precision\": model_precision,\n",
    "                     \"recall\": model_recall,\n",
    "                     \"f1\": model_f1}\n",
    "    return model_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 79.26509186351706,\n",
       " 'precision': 0.8111390004213173,\n",
       " 'recall': 0.7926509186351706,\n",
       " 'f1': 0.7862189758049549}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get baseline results\n",
    "baseline_results = calculate_results(val_labels, baseline_preds)\n",
    "baseline_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: A simple dense model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tensorboard callback (need to create a new one for each model)\n",
    "from helper_functions import create_tensorboard_callback\n",
    "\n",
    "# Create a directory to save tensorboard logs\n",
    "SAVE_DIR = \"model_logs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model with the Functional API\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "inputs = layers.Input(shape=(1,), dtype=tf.string) # inputs are 1-dimensional strings\n",
    "x = text_vectorizer(inputs) # turn the input text into numbers\n",
    "x = embedding(x) # create an embedding of the numberized inputs\n",
    "x = layers.GlobalAveragePooling1D()(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model_1 = tf.keras.Model(inputs, outputs, name=\"model_1_dense\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1_dense\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization_1 (Text  (None, 15)                0         \n",
      " Vectorization)                                                  \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 15, 128)           128000    \n",
      "                                                                 \n",
      " global_average_pooling1d (  (None, 128)               0         \n",
      " GlobalAveragePooling1D)                                         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 128129 (500.50 KB)\n",
      "Trainable params: 128129 (500.50 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complie model\n",
    "model_1.compile(loss=\"binary_crossentropy\",\n",
    "               optimizer=tf.keras.optimizers.Adam(),\n",
    "               metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving TensorBoard log files to: model_logs/model_1_dense/20231011-192313\n",
      "Epoch 1/5\n",
      "215/215 [==============================] - 8s 35ms/step - loss: 0.5979 - accuracy: 0.7116 - val_loss: 0.5488 - val_accuracy: 0.7559\n",
      "Epoch 2/5\n",
      "215/215 [==============================] - 3s 12ms/step - loss: 0.4750 - accuracy: 0.7983 - val_loss: 0.5012 - val_accuracy: 0.7651\n",
      "Epoch 3/5\n",
      "215/215 [==============================] - 2s 11ms/step - loss: 0.4272 - accuracy: 0.8158 - val_loss: 0.4915 - val_accuracy: 0.7769\n",
      "Epoch 4/5\n",
      "215/215 [==============================] - 2s 9ms/step - loss: 0.4034 - accuracy: 0.8229 - val_loss: 0.4865 - val_accuracy: 0.7782\n",
      "Epoch 5/5\n",
      "215/215 [==============================] - 2s 8ms/step - loss: 0.3891 - accuracy: 0.8264 - val_loss: 0.4917 - val_accuracy: 0.7769\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "model_1_hisotry = model_1.fit(x=train_sentences,\n",
    "                             y=train_labels,\n",
    "                             epochs=5,\n",
    "                             validation_data=(val_sentences, val_labels),\n",
    "                             callbacks=[create_tensorboard_callback(dir_name=SAVE_DIR,\n",
    "                                                                   experiment_name=\"model_1_dense\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 5ms/step - loss: 0.4917 - accuracy: 0.7769\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4917433559894562, 0.7769029140472412]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the results\n",
    "model_1.evaluate(val_sentences, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 3ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.581002  ],\n",
       "       [0.70840615],\n",
       "       [0.97309417],\n",
       "       [0.14050217],\n",
       "       [0.15727246],\n",
       "       [0.93611753],\n",
       "       [0.8648491 ],\n",
       "       [0.8543754 ],\n",
       "       [0.73574543],\n",
       "       [0.221249  ],\n",
       "       [0.32333896],\n",
       "       [0.5666261 ],\n",
       "       [0.04708002],\n",
       "       [0.21271738],\n",
       "       [0.03219694],\n",
       "       [0.16032262],\n",
       "       [0.03461066],\n",
       "       [0.27208093],\n",
       "       [0.2653989 ],\n",
       "       [0.35090697],\n",
       "       [0.7334534 ],\n",
       "       [0.03860169],\n",
       "       [0.17303199],\n",
       "       [0.10995845],\n",
       "       [0.61139834],\n",
       "       [0.9946163 ],\n",
       "       [0.0350367 ],\n",
       "       [0.02700869],\n",
       "       [0.06245933],\n",
       "       [0.13490885],\n",
       "       [0.6086232 ],\n",
       "       [0.46190807],\n",
       "       [0.37712458],\n",
       "       [0.4741216 ],\n",
       "       [0.38240924],\n",
       "       [0.04781082],\n",
       "       [0.9801195 ],\n",
       "       [0.16148731],\n",
       "       [0.04878677],\n",
       "       [0.9727192 ],\n",
       "       [0.25859714],\n",
       "       [0.06980623],\n",
       "       [0.35652098],\n",
       "       [0.14065813],\n",
       "       [0.49693343],\n",
       "       [0.9513442 ],\n",
       "       [0.16598025],\n",
       "       [0.8895065 ],\n",
       "       [0.2644429 ],\n",
       "       [0.59788513],\n",
       "       [0.03563579],\n",
       "       [0.6056845 ],\n",
       "       [0.35093287],\n",
       "       [0.02368289],\n",
       "       [0.04244731],\n",
       "       [0.0254697 ],\n",
       "       [0.24175577],\n",
       "       [0.79548657],\n",
       "       [0.16362882],\n",
       "       [0.00508565],\n",
       "       [0.18112093],\n",
       "       [0.95328623],\n",
       "       [0.8632467 ],\n",
       "       [0.15686606],\n",
       "       [0.9098009 ],\n",
       "       [0.9501123 ],\n",
       "       [0.7971833 ],\n",
       "       [0.6635295 ],\n",
       "       [0.11362146],\n",
       "       [0.27060425],\n",
       "       [0.04907507],\n",
       "       [0.03617097],\n",
       "       [0.69251275],\n",
       "       [0.11224544],\n",
       "       [0.19389245],\n",
       "       [0.37961894],\n",
       "       [0.36443633],\n",
       "       [0.48808604],\n",
       "       [0.27421984],\n",
       "       [0.74710464],\n",
       "       [0.5261667 ],\n",
       "       [0.32553494],\n",
       "       [0.98969275],\n",
       "       [0.06699352],\n",
       "       [0.30545023],\n",
       "       [0.060169  ],\n",
       "       [0.04954328],\n",
       "       [0.23429275],\n",
       "       [0.79848987],\n",
       "       [0.75267315],\n",
       "       [0.97459793],\n",
       "       [0.01223009],\n",
       "       [0.24354075],\n",
       "       [0.07814065],\n",
       "       [0.9042022 ],\n",
       "       [0.7159025 ],\n",
       "       [0.8144871 ],\n",
       "       [0.6788932 ],\n",
       "       [0.865721  ],\n",
       "       [0.94246906],\n",
       "       [0.97336835],\n",
       "       [0.2932206 ],\n",
       "       [0.07790237],\n",
       "       [0.85951847],\n",
       "       [0.86467105],\n",
       "       [0.04465749],\n",
       "       [0.795428  ],\n",
       "       [0.9324676 ],\n",
       "       [0.06913557],\n",
       "       [0.48446375],\n",
       "       [0.59000736],\n",
       "       [0.06152494],\n",
       "       [0.16591354],\n",
       "       [0.14388758],\n",
       "       [0.5448032 ],\n",
       "       [0.46011224],\n",
       "       [0.6170124 ],\n",
       "       [0.43178716],\n",
       "       [0.8096214 ],\n",
       "       [0.1000398 ],\n",
       "       [0.9988569 ],\n",
       "       [0.24135518],\n",
       "       [0.31777576],\n",
       "       [0.7408129 ],\n",
       "       [0.46027392],\n",
       "       [0.37603498],\n",
       "       [0.814563  ],\n",
       "       [0.0457574 ],\n",
       "       [0.17208152],\n",
       "       [0.8011587 ],\n",
       "       [0.13589807],\n",
       "       [0.9988569 ],\n",
       "       [0.9976165 ],\n",
       "       [0.9946163 ],\n",
       "       [0.981989  ],\n",
       "       [0.07040986],\n",
       "       [0.7144107 ],\n",
       "       [0.20019646],\n",
       "       [0.30973887],\n",
       "       [0.14021674],\n",
       "       [0.9822353 ],\n",
       "       [0.3467049 ],\n",
       "       [0.21271738],\n",
       "       [0.9112387 ],\n",
       "       [0.32825282],\n",
       "       [0.59485006],\n",
       "       [0.08542296],\n",
       "       [0.08709652],\n",
       "       [0.16929409],\n",
       "       [0.972064  ],\n",
       "       [0.21889493],\n",
       "       [0.11607774],\n",
       "       [0.41616896],\n",
       "       [0.5408481 ],\n",
       "       [0.28925   ],\n",
       "       [0.9685028 ],\n",
       "       [0.48679718],\n",
       "       [0.4541496 ],\n",
       "       [0.988186  ],\n",
       "       [0.11633376],\n",
       "       [0.9664151 ],\n",
       "       [0.02201352],\n",
       "       [0.22380464],\n",
       "       [0.93256044],\n",
       "       [0.26072356],\n",
       "       [0.46555063],\n",
       "       [0.9964702 ],\n",
       "       [0.33931988],\n",
       "       [0.81801516],\n",
       "       [0.28476003],\n",
       "       [0.9787202 ],\n",
       "       [0.73576653],\n",
       "       [0.654129  ],\n",
       "       [0.05755447],\n",
       "       [0.993853  ],\n",
       "       [0.13075973],\n",
       "       [0.5436474 ],\n",
       "       [0.56358314],\n",
       "       [0.73593575],\n",
       "       [0.9844838 ],\n",
       "       [0.05273803],\n",
       "       [0.6577968 ],\n",
       "       [0.6759894 ],\n",
       "       [0.70639664],\n",
       "       [0.9730917 ],\n",
       "       [0.6557988 ],\n",
       "       [0.12362397],\n",
       "       [0.9984334 ],\n",
       "       [0.25737756],\n",
       "       [0.10430066],\n",
       "       [0.07447172],\n",
       "       [0.95714355],\n",
       "       [0.27259603],\n",
       "       [0.34422553],\n",
       "       [0.06842001],\n",
       "       [0.14267875],\n",
       "       [0.06456809],\n",
       "       [0.24148196],\n",
       "       [0.52907234],\n",
       "       [0.08788117],\n",
       "       [0.25125703],\n",
       "       [0.5923404 ],\n",
       "       [0.946445  ],\n",
       "       [0.44433793],\n",
       "       [0.38878477],\n",
       "       [0.999647  ],\n",
       "       [0.28716767],\n",
       "       [0.79441804],\n",
       "       [0.5580918 ],\n",
       "       [0.9049777 ],\n",
       "       [0.34725937],\n",
       "       [0.97874284],\n",
       "       [0.07542016],\n",
       "       [0.3143276 ],\n",
       "       [0.0797654 ],\n",
       "       [0.03061006],\n",
       "       [0.66822004],\n",
       "       [0.71914047],\n",
       "       [0.8720937 ],\n",
       "       [0.12676536],\n",
       "       [0.69385743],\n",
       "       [0.16992085],\n",
       "       [0.03164272],\n",
       "       [0.24691921],\n",
       "       [0.9407849 ],\n",
       "       [0.17457682],\n",
       "       [0.4361816 ],\n",
       "       [0.8344093 ],\n",
       "       [0.42951387],\n",
       "       [0.5874689 ],\n",
       "       [0.4817207 ],\n",
       "       [0.26942107],\n",
       "       [0.75379074],\n",
       "       [0.24603556],\n",
       "       [0.60382885],\n",
       "       [0.1765    ],\n",
       "       [0.6147961 ],\n",
       "       [0.4602021 ],\n",
       "       [0.10417609],\n",
       "       [0.1693489 ],\n",
       "       [0.29596308],\n",
       "       [0.28882715],\n",
       "       [0.99945694],\n",
       "       [0.9601075 ],\n",
       "       [0.1127269 ],\n",
       "       [0.12480836],\n",
       "       [0.84173346],\n",
       "       [0.25641128],\n",
       "       [0.13437611],\n",
       "       [0.6325463 ],\n",
       "       [0.1804799 ],\n",
       "       [0.37784785],\n",
       "       [0.00643928],\n",
       "       [0.6828284 ],\n",
       "       [0.44339353],\n",
       "       [0.28698888],\n",
       "       [0.9120211 ],\n",
       "       [0.9790932 ],\n",
       "       [0.2852544 ],\n",
       "       [0.20531003],\n",
       "       [0.44657055],\n",
       "       [0.02111231],\n",
       "       [0.01560123],\n",
       "       [0.94834954],\n",
       "       [0.89122576],\n",
       "       [0.83509105],\n",
       "       [0.9302406 ],\n",
       "       [0.12325434],\n",
       "       [0.22305004],\n",
       "       [0.02717489],\n",
       "       [0.38304698],\n",
       "       [0.14299415],\n",
       "       [0.9331819 ],\n",
       "       [0.06747092],\n",
       "       [0.17788331],\n",
       "       [0.9550354 ],\n",
       "       [0.045791  ],\n",
       "       [0.11540622],\n",
       "       [0.9752966 ],\n",
       "       [0.17644718],\n",
       "       [0.06674248],\n",
       "       [0.00751865],\n",
       "       [0.77433634],\n",
       "       [0.44511005],\n",
       "       [0.79760444],\n",
       "       [0.8001509 ],\n",
       "       [0.28335056],\n",
       "       [0.08004448],\n",
       "       [0.90242994],\n",
       "       [0.03171453],\n",
       "       [0.7502009 ],\n",
       "       [0.33520415],\n",
       "       [0.3139919 ],\n",
       "       [0.4778144 ],\n",
       "       [0.3907962 ],\n",
       "       [0.7956535 ],\n",
       "       [0.22257547],\n",
       "       [0.86118   ],\n",
       "       [0.15681663],\n",
       "       [0.76437646],\n",
       "       [0.20526646],\n",
       "       [0.08146676],\n",
       "       [0.29224515],\n",
       "       [0.9316254 ],\n",
       "       [0.325568  ],\n",
       "       [0.17637588],\n",
       "       [0.35380456],\n",
       "       [0.26621836],\n",
       "       [0.11131129],\n",
       "       [0.04726167],\n",
       "       [0.19815876],\n",
       "       [0.95961535],\n",
       "       [0.43675372],\n",
       "       [0.25231755],\n",
       "       [0.9994467 ],\n",
       "       [0.04864527],\n",
       "       [0.67623156],\n",
       "       [0.29761767],\n",
       "       [0.06413078],\n",
       "       [0.19453225],\n",
       "       [0.48631483],\n",
       "       [0.16428019],\n",
       "       [0.92657214],\n",
       "       [0.3297916 ],\n",
       "       [0.899773  ],\n",
       "       [0.07298541],\n",
       "       [0.06235185],\n",
       "       [0.99362624],\n",
       "       [0.09755756],\n",
       "       [0.9853871 ],\n",
       "       [0.19082168],\n",
       "       [0.09996352],\n",
       "       [0.90608996],\n",
       "       [0.10901588],\n",
       "       [0.1351915 ],\n",
       "       [0.9557733 ],\n",
       "       [0.00814881],\n",
       "       [0.27569517],\n",
       "       [0.5881092 ],\n",
       "       [0.96397436],\n",
       "       [0.00202111],\n",
       "       [0.10510408],\n",
       "       [0.900736  ],\n",
       "       [0.9899278 ],\n",
       "       [0.64428616],\n",
       "       [0.42280215],\n",
       "       [0.5344218 ],\n",
       "       [0.28006312],\n",
       "       [0.06478964],\n",
       "       [0.27703676],\n",
       "       [0.1267343 ],\n",
       "       [0.9047213 ],\n",
       "       [0.10721315],\n",
       "       [0.40271026],\n",
       "       [0.42922777],\n",
       "       [0.06036711],\n",
       "       [0.9665885 ],\n",
       "       [0.9946163 ],\n",
       "       [0.9402802 ],\n",
       "       [0.04296447],\n",
       "       [0.44159183],\n",
       "       [0.13492617],\n",
       "       [0.31297794],\n",
       "       [0.70601225],\n",
       "       [0.11933499],\n",
       "       [0.03341544],\n",
       "       [0.08082193],\n",
       "       [0.04781684],\n",
       "       [0.58661157],\n",
       "       [0.01917511],\n",
       "       [0.10300385],\n",
       "       [0.11295871],\n",
       "       [0.30732873],\n",
       "       [0.538158  ],\n",
       "       [0.23664516],\n",
       "       [0.25956258],\n",
       "       [0.09784372],\n",
       "       [0.57674783],\n",
       "       [0.16002059],\n",
       "       [0.98203063],\n",
       "       [0.9132426 ],\n",
       "       [0.46211284],\n",
       "       [0.56637144],\n",
       "       [0.2117176 ],\n",
       "       [0.43636566],\n",
       "       [0.9790243 ],\n",
       "       [0.73024136],\n",
       "       [0.12320884],\n",
       "       [0.9831972 ],\n",
       "       [0.25570634],\n",
       "       [0.3098777 ],\n",
       "       [0.46081328],\n",
       "       [0.03130225],\n",
       "       [0.54149336],\n",
       "       [0.54226327],\n",
       "       [0.9937816 ],\n",
       "       [0.11261706],\n",
       "       [0.14313026],\n",
       "       [0.10399602],\n",
       "       [0.11933499],\n",
       "       [0.98573595],\n",
       "       [0.05443718],\n",
       "       [0.7143669 ],\n",
       "       [0.9237815 ],\n",
       "       [0.5122036 ],\n",
       "       [0.9988569 ],\n",
       "       [0.08655927],\n",
       "       [0.6795276 ],\n",
       "       [0.05586201],\n",
       "       [0.60929304],\n",
       "       [0.85904723],\n",
       "       [0.03845344],\n",
       "       [0.00142535],\n",
       "       [0.05572651],\n",
       "       [0.95066684],\n",
       "       [0.8418041 ],\n",
       "       [0.5386874 ],\n",
       "       [0.606382  ],\n",
       "       [0.7427074 ],\n",
       "       [0.10440865],\n",
       "       [0.8066506 ],\n",
       "       [0.65868545],\n",
       "       [0.994375  ],\n",
       "       [0.8654807 ],\n",
       "       [0.16027787],\n",
       "       [0.31606564],\n",
       "       [0.1583645 ],\n",
       "       [0.60436225],\n",
       "       [0.41826114],\n",
       "       [0.44250312],\n",
       "       [0.03624473],\n",
       "       [0.37088063],\n",
       "       [0.07098891],\n",
       "       [0.22570997],\n",
       "       [0.05857179],\n",
       "       [0.6777966 ],\n",
       "       [0.36761692],\n",
       "       [0.30184048],\n",
       "       [0.98933816],\n",
       "       [0.93304074],\n",
       "       [0.3184779 ],\n",
       "       [0.70840615],\n",
       "       [0.34190604],\n",
       "       [0.03773853],\n",
       "       [0.35993537],\n",
       "       [0.6604926 ],\n",
       "       [0.05914358],\n",
       "       [0.20456727],\n",
       "       [0.25882888],\n",
       "       [0.5754551 ],\n",
       "       [0.00781637],\n",
       "       [0.8508088 ],\n",
       "       [0.87093544],\n",
       "       [0.9910772 ],\n",
       "       [0.9834682 ],\n",
       "       [0.790325  ],\n",
       "       [0.14558071],\n",
       "       [0.29419085],\n",
       "       [0.6972684 ],\n",
       "       [0.86774284],\n",
       "       [0.9874423 ],\n",
       "       [0.012712  ],\n",
       "       [0.15322605],\n",
       "       [0.41896552],\n",
       "       [0.9920402 ],\n",
       "       [0.9988224 ],\n",
       "       [0.25979248],\n",
       "       [0.05511111],\n",
       "       [0.9949285 ],\n",
       "       [0.06335024],\n",
       "       [0.51555794],\n",
       "       [0.812799  ],\n",
       "       [0.2924705 ],\n",
       "       [0.08100092],\n",
       "       [0.9212836 ],\n",
       "       [0.34067577],\n",
       "       [0.40395394],\n",
       "       [0.66311485],\n",
       "       [0.0199926 ],\n",
       "       [0.09393591],\n",
       "       [0.16835225],\n",
       "       [0.03079433],\n",
       "       [0.1803531 ],\n",
       "       [0.8536343 ],\n",
       "       [0.06096939],\n",
       "       [0.28855327],\n",
       "       [0.46286228],\n",
       "       [0.07397126],\n",
       "       [0.45375684],\n",
       "       [0.2982721 ],\n",
       "       [0.4776801 ],\n",
       "       [0.9823439 ],\n",
       "       [0.24048014],\n",
       "       [0.10216203],\n",
       "       [0.3819242 ],\n",
       "       [0.13695318],\n",
       "       [0.0815347 ],\n",
       "       [0.5386725 ],\n",
       "       [0.0475938 ],\n",
       "       [0.8894462 ],\n",
       "       [0.8702608 ],\n",
       "       [0.67125255],\n",
       "       [0.6321369 ],\n",
       "       [0.8373986 ],\n",
       "       [0.0502853 ],\n",
       "       [0.3794816 ],\n",
       "       [0.31453043],\n",
       "       [0.875172  ],\n",
       "       [0.10896033],\n",
       "       [0.28112343],\n",
       "       [0.5093862 ],\n",
       "       [0.1500442 ],\n",
       "       [0.29138628],\n",
       "       [0.29543492],\n",
       "       [0.87909293],\n",
       "       [0.15113987],\n",
       "       [0.9850545 ],\n",
       "       [0.69120204],\n",
       "       [0.7159025 ],\n",
       "       [0.97824335],\n",
       "       [0.17781813],\n",
       "       [0.1394598 ],\n",
       "       [0.8751227 ],\n",
       "       [0.04768711],\n",
       "       [0.02230724],\n",
       "       [0.09817091],\n",
       "       [0.05039025],\n",
       "       [0.10682914],\n",
       "       [0.8980257 ],\n",
       "       [0.69402254],\n",
       "       [0.76381344],\n",
       "       [0.89836633],\n",
       "       [0.07277889],\n",
       "       [0.12485471],\n",
       "       [0.64006543],\n",
       "       [0.02812236],\n",
       "       [0.32434487],\n",
       "       [0.10217489],\n",
       "       [0.31207335],\n",
       "       [0.46561137],\n",
       "       [0.06953802],\n",
       "       [0.08429662],\n",
       "       [0.6068946 ],\n",
       "       [0.46199003],\n",
       "       [0.37212655],\n",
       "       [0.26445776],\n",
       "       [0.49038693],\n",
       "       [0.9943916 ],\n",
       "       [0.99137586],\n",
       "       [0.4413416 ],\n",
       "       [0.82779217],\n",
       "       [0.9807719 ],\n",
       "       [0.00133902],\n",
       "       [0.68718797],\n",
       "       [0.19235478],\n",
       "       [0.19148871],\n",
       "       [0.27354005],\n",
       "       [0.17735665],\n",
       "       [0.2658426 ],\n",
       "       [0.12044091],\n",
       "       [0.14846005],\n",
       "       [0.66230345],\n",
       "       [0.7731461 ],\n",
       "       [0.48638403],\n",
       "       [0.9908766 ],\n",
       "       [0.05013352],\n",
       "       [0.6370402 ],\n",
       "       [0.4856667 ],\n",
       "       [0.03871237],\n",
       "       [0.18001765],\n",
       "       [0.9214242 ],\n",
       "       [0.82900983],\n",
       "       [0.95551646],\n",
       "       [0.22580148],\n",
       "       [0.11361381],\n",
       "       [0.36940297],\n",
       "       [0.55103093],\n",
       "       [0.3980796 ],\n",
       "       [0.9807719 ],\n",
       "       [0.29952276],\n",
       "       [0.11879485],\n",
       "       [0.12516724],\n",
       "       [0.98993266],\n",
       "       [0.25831923],\n",
       "       [0.07738236],\n",
       "       [0.5487046 ],\n",
       "       [0.3042149 ],\n",
       "       [0.05187938],\n",
       "       [0.23747334],\n",
       "       [0.22603887],\n",
       "       [0.20522472],\n",
       "       [0.33303782],\n",
       "       [0.19630176],\n",
       "       [0.14554326],\n",
       "       [0.09380627],\n",
       "       [0.28068075],\n",
       "       [0.0889601 ],\n",
       "       [0.9100451 ],\n",
       "       [0.65560234],\n",
       "       [0.39114726],\n",
       "       [0.06823163],\n",
       "       [0.01053893],\n",
       "       [0.97918975],\n",
       "       [0.57949144],\n",
       "       [0.9989165 ],\n",
       "       [0.27896267],\n",
       "       [0.8085223 ],\n",
       "       [0.17119794],\n",
       "       [0.32703337],\n",
       "       [0.599959  ],\n",
       "       [0.05147795],\n",
       "       [0.90612304],\n",
       "       [0.11401533],\n",
       "       [0.3865004 ],\n",
       "       [0.9902354 ],\n",
       "       [0.11052997],\n",
       "       [0.07724551],\n",
       "       [0.20748146],\n",
       "       [0.0453332 ],\n",
       "       [0.45893717],\n",
       "       [0.999647  ],\n",
       "       [0.1639181 ],\n",
       "       [0.9285203 ],\n",
       "       [0.23160364],\n",
       "       [0.64956397],\n",
       "       [0.16862564],\n",
       "       [0.28785008],\n",
       "       [0.26008904],\n",
       "       [0.57972515],\n",
       "       [0.03881384],\n",
       "       [0.23514493],\n",
       "       [0.5931051 ],\n",
       "       [0.55155605],\n",
       "       [0.9780862 ],\n",
       "       [0.3734561 ],\n",
       "       [0.02765821],\n",
       "       [0.30291912],\n",
       "       [0.05571956],\n",
       "       [0.6470868 ],\n",
       "       [0.27894437],\n",
       "       [0.8184688 ],\n",
       "       [0.08027114],\n",
       "       [0.63964874],\n",
       "       [0.6728278 ],\n",
       "       [0.50184137],\n",
       "       [0.27288535],\n",
       "       [0.28112343],\n",
       "       [0.28778216],\n",
       "       [0.38268262],\n",
       "       [0.5572686 ],\n",
       "       [0.99435365],\n",
       "       [0.06963934],\n",
       "       [0.04134858],\n",
       "       [0.05641117],\n",
       "       [0.19377862],\n",
       "       [0.23177478],\n",
       "       [0.0246282 ],\n",
       "       [0.5933148 ],\n",
       "       [0.20528562],\n",
       "       [0.23925819],\n",
       "       [0.1980373 ],\n",
       "       [0.25833002],\n",
       "       [0.9088273 ],\n",
       "       [0.17131029],\n",
       "       [0.5825819 ],\n",
       "       [0.31167644],\n",
       "       [0.14206226],\n",
       "       [0.28711674],\n",
       "       [0.9992644 ],\n",
       "       [0.80676776],\n",
       "       [0.0192264 ],\n",
       "       [0.24882396],\n",
       "       [0.16807778],\n",
       "       [0.31658584],\n",
       "       [0.7508515 ],\n",
       "       [0.6091133 ],\n",
       "       [0.6170325 ],\n",
       "       [0.2398937 ],\n",
       "       [0.30943984],\n",
       "       [0.759391  ],\n",
       "       [0.17954224],\n",
       "       [0.05360286],\n",
       "       [0.8487367 ],\n",
       "       [0.17951398],\n",
       "       [0.30094653],\n",
       "       [0.9820386 ],\n",
       "       [0.1300301 ],\n",
       "       [0.45014167],\n",
       "       [0.10682914],\n",
       "       [0.33180025],\n",
       "       [0.9019583 ],\n",
       "       [0.9988569 ],\n",
       "       [0.5428737 ],\n",
       "       [0.03710644],\n",
       "       [0.9890015 ],\n",
       "       [0.50714856],\n",
       "       [0.29931036],\n",
       "       [0.50714856],\n",
       "       [0.909101  ],\n",
       "       [0.07665101],\n",
       "       [0.61726457],\n",
       "       [0.12375432],\n",
       "       [0.92752004],\n",
       "       [0.40993547],\n",
       "       [0.7059674 ],\n",
       "       [0.07111741],\n",
       "       [0.43242157],\n",
       "       [0.07263548],\n",
       "       [0.03839492],\n",
       "       [0.2635939 ],\n",
       "       [0.07378815],\n",
       "       [0.08642455],\n",
       "       [0.79497033],\n",
       "       [0.02508995],\n",
       "       [0.1458203 ],\n",
       "       [0.060684  ],\n",
       "       [0.1672647 ],\n",
       "       [0.09785351],\n",
       "       [0.67800075],\n",
       "       [0.07329378],\n",
       "       [0.66865456],\n",
       "       [0.07495713],\n",
       "       [0.12726474],\n",
       "       [0.45406783],\n",
       "       [0.37865412],\n",
       "       [0.12329916],\n",
       "       [0.27420068],\n",
       "       [0.03172639],\n",
       "       [0.94991446],\n",
       "       [0.1672647 ],\n",
       "       [0.43529227],\n",
       "       [0.9694793 ],\n",
       "       [0.97324705],\n",
       "       [0.9870062 ],\n",
       "       [0.9995447 ],\n",
       "       [0.9903846 ],\n",
       "       [0.30978778],\n",
       "       [0.23113652],\n",
       "       [0.2854376 ],\n",
       "       [0.50031054],\n",
       "       [0.9939042 ],\n",
       "       [0.44180992],\n",
       "       [0.34929946],\n",
       "       [0.40426615],\n",
       "       [0.5419038 ],\n",
       "       [0.09438948],\n",
       "       [0.25577813],\n",
       "       [0.0853881 ],\n",
       "       [0.28109297],\n",
       "       [0.00788059],\n",
       "       [0.09231107],\n",
       "       [0.24815789],\n",
       "       [0.88628626],\n",
       "       [0.04454602],\n",
       "       [0.67782974],\n",
       "       [0.8093121 ],\n",
       "       [0.05232071],\n",
       "       [0.3141107 ],\n",
       "       [0.09853186],\n",
       "       [0.76064724],\n",
       "       [0.44459608],\n",
       "       [0.01032301]], dtype=float32)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make some predictions and evalueate those\n",
    "model_1_pred_probs = model_1.predict(val_sentences)\n",
    "model_1_pred_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(762,), dtype=float32, numpy=\n",
       "array([1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "       0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0.,\n",
       "       1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1.,\n",
       "       0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0.,\n",
       "       0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
       "       0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1.,\n",
       "       0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0.,\n",
       "       1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0.,\n",
       "       1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1., 0.,\n",
       "       1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
       "       1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0.,\n",
       "       0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1.,\n",
       "       0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 0.,\n",
       "       0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0.,\n",
       "       0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0.,\n",
       "       1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1.,\n",
       "       0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1.,\n",
       "       0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 1.,\n",
       "       0., 0., 1., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1.,\n",
       "       1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1.,\n",
       "       0., 0., 0., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0., 0.,\n",
       "       0., 0., 1., 1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1.,\n",
       "       0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0.,\n",
       "       0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 1.,\n",
       "       0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
       "       1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 1., 0., 0., 1., 0.,\n",
       "       0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "       0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 0.,\n",
       "       1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0.,\n",
       "       0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "       0., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       1., 1., 0., 1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 0.,\n",
       "       1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1.,\n",
       "       0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1.,\n",
       "       0., 0., 1., 1., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 1., 0.,\n",
       "       0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1.,\n",
       "       0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0., 1.,\n",
       "       0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0.,\n",
       "       1., 1., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "       0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "       1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 0.],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert model prediction probabilities to label format\n",
    "model_1_preds = tf.squeeze(tf.round(model_1_pred_probs))\n",
    "model_1_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 77.69028871391076,\n",
       " 'precision': 0.7802261455878201,\n",
       " 'recall': 0.7769028871391076,\n",
       " 'f1': 0.7742088128407028}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate our model_1 results\n",
    "model_1_results = calculate_results(val_labels,\n",
    "                                   model_1_preds)\n",
    "model_1_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 79.26509186351706,\n",
       " 'precision': 0.8111390004213173,\n",
       " 'recall': 0.7926509186351706,\n",
       " 'f1': 0.7862189758049549}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, False])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.array(list(model_1_results.values())) > np.array(list(baseline_results.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
